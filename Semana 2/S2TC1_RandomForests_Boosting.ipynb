{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image info](https://raw.githubusercontent.com/albahnsen/MIAD_ML_and_NLP/main/images/banner_1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Taller: Construcción e implementación de modelos Bagging, Random Forest y XGBoost\n",
    "\n",
    "En este taller podrán poner en práctica sus conocimientos sobre la construcción e implementación de modelos de Bagging, Random Forest y XGBoost. El taller está constituido por 8 puntos, en los cuales deberan seguir las intrucciones de cada numeral para su desarrollo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datos predicción precio de automóviles\n",
    "\n",
    "En este taller se usará el conjunto de datos de Car Listings de Kaggle donde cada observación representa el precio de un automóvil teniendo en cuenta distintas variables como año, marca, modelo, entre otras. El objetivo es predecir el precio del automóvil. Para más detalles puede visitar el siguiente enlace: [datos](https://www.kaggle.com/jpayne/852k-used-car-listings)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Price</th>\n",
       "      <th>Year</th>\n",
       "      <th>Mileage</th>\n",
       "      <th>M_Camry</th>\n",
       "      <th>M_Camry4dr</th>\n",
       "      <th>M_CamryBase</th>\n",
       "      <th>M_CamryL</th>\n",
       "      <th>M_CamryLE</th>\n",
       "      <th>M_CamrySE</th>\n",
       "      <th>M_CamryXLE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>21995</td>\n",
       "      <td>2014</td>\n",
       "      <td>6480</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>13995</td>\n",
       "      <td>2014</td>\n",
       "      <td>39972</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>167</th>\n",
       "      <td>17941</td>\n",
       "      <td>2016</td>\n",
       "      <td>18989</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>225</th>\n",
       "      <td>12493</td>\n",
       "      <td>2014</td>\n",
       "      <td>51330</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>270</th>\n",
       "      <td>7994</td>\n",
       "      <td>2007</td>\n",
       "      <td>116065</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Price  Year  Mileage  M_Camry  M_Camry4dr  M_CamryBase  M_CamryL   \n",
       "7    21995  2014     6480        0           0            0         1  \\\n",
       "11   13995  2014    39972        0           0            0         0   \n",
       "167  17941  2016    18989        0           0            0         0   \n",
       "225  12493  2014    51330        0           0            0         1   \n",
       "270   7994  2007   116065        0           1            0         0   \n",
       "\n",
       "     M_CamryLE  M_CamrySE  M_CamryXLE  \n",
       "7            0          0           0  \n",
       "11           1          0           0  \n",
       "167          0          1           0  \n",
       "225          0          0           0  \n",
       "270          0          0           0  "
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Importación de librerías\n",
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "# Lectura de la información de archivo .csv\n",
    "data = pd.read_csv('https://raw.githubusercontent.com/albahnsen/MIAD_ML_and_NLP/main/datasets/dataTrain_carListings.zip')\n",
    "\n",
    "# Preprocesamiento de datos para el taller\n",
    "data = data.loc[data['Model'].str.contains('Camry')].drop(['Make', 'State'], axis=1)\n",
    "data = data.join(pd.get_dummies(data['Model'], prefix='M'))\n",
    "data = data.drop(['Model'], axis=1)\n",
    "data=data.astype(int)\n",
    "\n",
    "# Visualización dataset\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separación de variables predictoras (X) y variable de interés (y)\n",
    "y = data['Price']\n",
    "X = data.drop(['Price'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separación de datos en set de entrenamiento y test\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Punto 1 - Árbol de decisión manual\n",
    "\n",
    "En la celda 1 creen un árbol de decisión **manualmente**  que considere los set de entrenamiento y test definidos anteriormente y presenten el RMSE y MAE del modelo en el set de test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función de error cuadrático medio (MSE)\n",
    "def mse(y):\n",
    "    return np.mean((y - y.mean()) ** 2)\n",
    "\n",
    "# Función de impureza utilizando MSE\n",
    "def mse_impurity(X_col, y, split):\n",
    "    filter_l = X_col < split\n",
    "    y_l = y.loc[filter_l]\n",
    "    y_r = y.loc[~filter_l]\n",
    "    n_l = y_l.shape[0]\n",
    "    n_r = y_r.shape[0]\n",
    "    mse_y = mse(y)\n",
    "    mse_l = mse(y_l)\n",
    "    mse_r = mse(y_r)\n",
    "    mse_impurity_ = mse_y - (n_l / (n_l + n_r) * mse_l + n_r / (n_l + n_r) * mse_r)\n",
    "    return mse_impurity_\n",
    "\n",
    "# Función para encontrar la mejor división\n",
    "def best_split(X, y, num_pct=10):\n",
    "    features = range(X.shape[1])\n",
    "    best_split = [0, 0, 0]  # j, split, gain  \n",
    "    # Para todas las variables \n",
    "    for j in features:\n",
    "        splits = np.percentile(X.iloc[:, j], np.arange(0, 100, 100.0 / (num_pct+1)).tolist())\n",
    "        splits = np.unique(splits)[1:]\n",
    "        # Para cada partición\n",
    "        for split in splits:\n",
    "            gain = mse_impurity(X.iloc[:, j], y, split)   \n",
    "            if gain > best_split[2]:\n",
    "                best_split = [j, split, gain]\n",
    "    return best_split\n",
    "\n",
    "# Función para hacer crecer el árbol de regresión\n",
    "def tree_grow(X, y, level=0, min_gain=0.001, max_depth=None, num_pct=10):\n",
    "    # Si solo es una observación\n",
    "    if X.shape[0] == 1:\n",
    "        tree = dict(y_pred=y.iloc[:1].values[0], level=level, split=-1, n_samples=1, gain=0)\n",
    "        return tree\n",
    "    # Calcular la mejor división\n",
    "    j, split, gain = best_split(X, y, num_pct)\n",
    "    # Guardar el árbol y estimar la predicción\n",
    "    y_pred = y.mean()\n",
    "    tree = dict(y_pred=y_pred, level=level, split=-1, n_samples=X.shape[0], gain=gain)\n",
    "    # Revisar el criterio de parada \n",
    "    if gain < min_gain:\n",
    "        return tree\n",
    "    if max_depth is not None:\n",
    "        if level >= max_depth:\n",
    "            return tree   \n",
    "    # Continuar creando la partición\n",
    "    filter_l = X.iloc[:, j] < split\n",
    "    X_l, y_l = X.loc[filter_l], y.loc[filter_l]\n",
    "    X_r, y_r = X.loc[~filter_l], y.loc[~filter_l]\n",
    "    tree['split'] = [j, split]\n",
    "    # Siguiente iteración para cada partición\n",
    "    tree['sl'] = tree_grow(X_l, y_l, level + 1, min_gain=min_gain, max_depth=max_depth, num_pct=num_pct)\n",
    "    tree['sr'] = tree_grow(X_r, y_r, level + 1, min_gain=min_gain, max_depth=max_depth, num_pct=num_pct)\n",
    "    return tree\n",
    "\n",
    "# Función para realizar predicciones con el árbol entrenado\n",
    "def tree_predict(X, tree):\n",
    "    predicted = np.zeros(X.shape[0])\n",
    "    if tree['split'] == -1:\n",
    "        predicted[:] = tree['y_pred']\n",
    "    else:\n",
    "        j, split = tree['split']\n",
    "        filter_l = (X.iloc[:, j] < split)\n",
    "        X_l = X.loc[filter_l]\n",
    "        X_r = X.loc[~filter_l]\n",
    "        if X_l.shape[0] == 0:\n",
    "            predicted[~filter_l] = tree_predict(X_r, tree['sr'])\n",
    "        elif X_r.shape[0] == 0:\n",
    "            predicted[filter_l] = tree_predict(X_l, tree['sl'])\n",
    "        else:\n",
    "            predicted[filter_l] = tree_predict(X_l, tree['sl'])\n",
    "            predicted[~filter_l] = tree_predict(X_r, tree['sr'])\n",
    "    return predicted.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 1643.317917244335 MAE: 1223.661662817552\n"
     ]
    }
   ],
   "source": [
    "tree=tree_grow(X_train, y_train, level=0, min_gain=0.001, max_depth=6, num_pct=10)\n",
    "prediction = tree_predict(X_test, tree)\n",
    "\n",
    "rmse = np.sqrt(mean_squared_error(y_test, prediction))\n",
    "mae = mean_absolute_error(y_test, prediction)\n",
    "\n",
    "print(\"RMSE:\", rmse, \"MAE:\", mae)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Punto 2 - Bagging manual\n",
    "\n",
    "En la celda 2 creen un modelo bagging **manualmente** con 10 árboles de regresión y comenten sobre el desempeño del modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([3582, 3454, 1346, ...,  826,  801, 5657]),\n",
       " array([6962, 3408, 2553, ..., 6611,  877, 6412]),\n",
       " array([1917, 3131,  384, ..., 2876, 6449, 6557]),\n",
       " array([3849, 4565, 6820, ..., 6835, 4643,  639]),\n",
       " array([2468, 3608, 1367, ..., 3108, 2961, 4357]),\n",
       " array([3142, 1537, 5966, ..., 3224, 6922, 3396]),\n",
       " array([6588, 3753, 1786, ..., 2131, 4627, 5672]),\n",
       " array([1289, 3776,  981, ..., 1891, 6034, 6165]),\n",
       " array([6079, 3819, 6976, ..., 6478, 2225, 5072]),\n",
       " array([3952, 3276,  896, ..., 4589, 1964,  893])]"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Punto 3 - Bagging con librería\n",
    "\n",
    "En la celda 3, con la librería sklearn, entrenen un modelo bagging con 10 árboles de regresión y el parámetro `max_features` igual a `log(n_features)` y comenten sobre el desempeño del modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Celda 3\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Punto 4 - Random forest con librería\n",
    "\n",
    "En la celda 4, usando la librería sklearn entrenen un modelo de Randon Forest para regresión  y comenten sobre el desempeño del modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Celda 4\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Punto 5 - Calibración de parámetros Random forest\n",
    "\n",
    "En la celda 5, calibren los parámetros max_depth, max_features y n_estimators del modelo de Randon Forest para regresión, comenten sobre el desempeño del modelo y describan cómo cada parámetro afecta el desempeño del modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Celda 5\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Punto 6 - XGBoost con librería\n",
    "\n",
    "En la celda 6 implementen un modelo XGBoost de regresión con la librería sklearn y comenten sobre el desempeño del modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Celda 6\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Punto 7 - Calibración de parámetros XGBoost\n",
    "\n",
    "En la celda 7 calibren los parámetros learning rate, gamma y colsample_bytree del modelo XGBoost para regresión, comenten sobre el desempeño del modelo y describan cómo cada parámetro afecta el desempeño del modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Celda 7\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Punto 8 - Comparación y análisis de resultados\n",
    "En la celda 8 comparen los resultados obtenidos de los diferentes modelos (random forest y XGBoost) y comenten las ventajas del mejor modelo y las desventajas del modelo con el menor desempeño."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Celda 8\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
